{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collector functions \n",
    "\n",
    "def get_movieList (path) :\n",
    "    movies = pd.DataFrame(pd.read_html(path + \"\\\\movies1.html\")[0]) #put the content of html file in a dataframe and get the first column\n",
    "    movies.drop('Id', inplace=True, axis = 1)\n",
    "    return movies\n",
    "\n",
    "def save_html(movies) :\n",
    "    for i in range(len(movies)):\n",
    "        try:\n",
    "            response = rq.get(movies.URL[i])\n",
    "        except rq.exceptions.RequestException as e: #if we got blocked by wiki we apply a time sleep\n",
    "            print(e)\n",
    "            time.sleep(20*60 + 30)\n",
    "            response = rq.get(movies.URL[i])\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        f = open('article_'+str(i)+'.html','w')\n",
    "        f.write(str(soup))\n",
    "        f.close()\n",
    "        time.sleep(random.choice(range(1,6)) #time sleep between each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating index functions\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text) #devide the text into substrings\n",
    "    filtered1 = [w for w in words if not w in stop_words] #remove stop words\n",
    "    filtered2 = list(filter(lambda word: word not in string.punctuation, filtered1))\n",
    "    filtered3 = []\n",
    "    for word in filtered2:\n",
    "        try:\n",
    "            filtered3 += re.findall(r'\\w+', word) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    filtered3 = [stemmer.stem(w) for w in filtered3] #stemming\n",
    "    filtered4 = [c.replace(\"''\", \"\").replace(\"``\", \"\") for c in filtered3 ] #removing useless '' and  `` characters\n",
    "    filtered4 = [f for f in filtered4 if len(f)>1]\n",
    "    return filtered4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetojson(pathfile, obj):\n",
    "    with open(pathfile, \"w\" ,encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(json.dumps(obj, ensure_ascii = False))\n",
    "        out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_index(path) :\n",
    "    allwords = list()\n",
    "    docwords = dict()\n",
    "    tsvs = dict()\n",
    "    vocabulary = dict()\n",
    "    for i in range(0,30000):\n",
    "        with open(path+\"\\\\TSV\\\\article_\" + str(i) + \".tsv\", encoding = \"utf-8\") as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row :\n",
    "                    tsv = row\n",
    "        text = ' '.join([tsv[1],tsv[2]])\n",
    "        tsvs[i] = tsv\n",
    "        cleared = clean(text)\n",
    "\n",
    "        docwords['document_'+str(i)] = cleared\n",
    "        allwords += cleared\n",
    "        \n",
    "        \n",
    "    allwords = list(set(allwords))\n",
    "        for i in range(len(allwords)):\n",
    "            vocabulary[str(i)] = allwords[i]\n",
    "            \n",
    "            \n",
    "            \n",
    "    savetojson(path+\"\\\\tsvs.json\", tsvs)\n",
    "    savetojson(path + \"\\\\WORDS\\\\DocWords.json\", docwords)\n",
    "    savetojson(path + \"\\\\WORDS\\\\vocabulary.json\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(path) :\n",
    "    inverted = defaultdict(list)\n",
    "    \n",
    "    with open(path + \"\\\\WORDS\\\\vocabulary.json\", encoding = \"utf-8\") as fd:\n",
    "        vocabulary = json.load(fd)\n",
    "        \n",
    "    reverse_voc = {v:k for k,v in vocabulary.items()}\n",
    "    \n",
    "    for doc in docwords.keys():\n",
    "        for word in docwords[doc]:\n",
    "            if not doc in inverted[reverse_voc[word]]:\n",
    "                inverted[reverse_voc[word]].append(doc)\n",
    "                \n",
    "    savetojson(path + \"\\\\WORDS\\\\Inverted_index.json\", inverted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
