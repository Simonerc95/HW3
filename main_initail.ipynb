{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Group 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Get the list of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we get the list of all movies urls in the movies html files. Each member of the group used this code to get the list of movies and then html files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieList (path) :\n",
    "    movies = pd.DataFrame(pd.read_html(path + \"\\\\movies1.html\")[0]) #put the content of html file in a dataframe and get the first column\n",
    "    movies.drop('Id', inplace=True, axis = 1)\n",
    "    return movies\n",
    "path = os.getcwd() #The address of directory where Movies.html files exist\n",
    "movies = get_movieList(path)  #this function will give us list of movies urls in the html file of movies which exist in the path address   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we crawl each wikipedia page to get html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(movies) :\n",
    "    for i in range(len(movies)):\n",
    "        try:\n",
    "            response = rq.get(movies.URL[i])\n",
    "        except rq.exceptions.RequestException as e: #if we got blocked by wiki we apply a time sleep\n",
    "            print(e)\n",
    "            time.sleep(20*60 + 30)\n",
    "            response = rq.get(movies.URL[i])\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        f = open('article_'+str(i)+'.html','w')\n",
    "        f.write(str(soup))\n",
    "        f.close()\n",
    "        time.sleep(random.choice(range(1,6)) #time sleep between each request\n",
    "                   \n",
    "save_html(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we should parse HTML pages, get the specefic information we want and then save it as TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the TSV Files were preprocessed by :\n",
    "1) Tokenization\n",
    "\n",
    "2) Removing stop words\n",
    "\n",
    "3) Removing punctuation\n",
    "\n",
    "4) Stemming\n",
    "\n",
    "5) Removing [] , \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stemmer = PorterStemmer()\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text) #devide the text into substrings\n",
    "    filtered1 = [w for w in words if not w in stop_words] #remove stop words\n",
    "    filtered2 = list(filter(lambda word: word not in string.punctuation, filtered1))\n",
    "    filtered3 = []\n",
    "    for word in filtered2:\n",
    "        try:\n",
    "            filtered3 += re.findall(r'\\w+', word) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    filtered3 = [stemmer.stem(w) for w in filtered3] #stemming\n",
    "    filtered4 = [c.replace(\"''\", \"\").replace(\"``\", \"\") for c in filtered3 ] #removing useless '' and  `` characters\n",
    "    filtered4 = [f for f in filtered4 if len(f)>1]\n",
    "    return filtered4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) creating index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function save an object to desired path as a json file\n",
    "def savetojson(pathfile, obj):\n",
    "    with open(pathfile, \"w\" ,encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(json.dumps(obj, ensure_ascii = False))\n",
    "        out_file.close()\n",
    "        \n",
    "        \n",
    "#this function gives us 3 json files, first the vocabulary which contains a dictionary of term ids and words\n",
    "#second the tsvs which containd a dictionary of cleared text of intro and plot for each document\n",
    "#third docwords which contains a dictionary of tokens of each document\n",
    "def get_vocab_index(path) :\n",
    "    allwords = list()\n",
    "    docwords = dict() # point each document to its containing words\n",
    "    tsvs = dict()\n",
    "    vocabulary = dict() # point each term id to a word\n",
    "    for i in range(0,30000):\n",
    "        with open(path+\"\\\\TSV\\\\article_\" + str(i) + \".tsv\", encoding = \"utf-8\") as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row :\n",
    "                    tsv = row\n",
    "        text = ' '.join([tsv[1],tsv[2]]) #get intro and plot of each tsv file\n",
    "        tsvs[i] = tsv\n",
    "        cleared = clean(text)\n",
    "\n",
    "        docwords['document_'+str(i)] = cleared\n",
    "        allwords += cleared\n",
    "        \n",
    "        \n",
    "    allwords = list(set(allwords)) # get the list of unique words\n",
    "        for i in range(len(allwords)):\n",
    "            vocabulary[str(i)] = allwords[i]\n",
    "            \n",
    "            \n",
    "            \n",
    "    savetojson(path+\"\\\\tsvs.json\", tsvs)\n",
    "    savetojson(path + \"\\\\WORDS\\\\DocWords.json\", docwords)\n",
    "    savetojson(path + \"\\\\WORDS\\\\vocabulary.json\", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) execute query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2)Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a new score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
