{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Group 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests as rq\n",
    "import time\n",
    "import random\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "import string\n",
    "import re\n",
    "from math import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "import heapq\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Get the list of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we get the list of all movies urls in the movies html files. Each member of the group used this code to get the list of movies and then html files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieList (path) :\n",
    "    movies = pd.DataFrame(pd.read_html(path + \"\\\\movies1.html\")[0]) #put the content of html file in a dataframe and get the first column\n",
    "    movies.drop('Id', inplace=True, axis = 1)\n",
    "    return movies\n",
    "path = os.getcwd() #The address of directory where Movies.html files exist\n",
    "movies = get_movieList(path)  #this function will give us list of movies urls in the html file of movies which exist in the path address   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we crawl each wikipedia page to get html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(movies) :\n",
    "    for i in range(len(movies)):\n",
    "        try:\n",
    "            response = rq.get(movies.URL[i])\n",
    "        except rq.exceptions.RequestException as e: #if we got blocked by wiki we apply a time sleep\n",
    "            print(e)\n",
    "            time.sleep(20*60 + 30)\n",
    "            response = rq.get(movies.URL[i])\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        f = open('article_'+str(i)+'.html','w')\n",
    "        f.write(str(soup))\n",
    "        f.close()\n",
    "        time.sleep(random.choice(range(1,6)) #time sleep between each request\n",
    "                   \n",
    "save_html(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we should parse HTML pages, get the specefic information we want and then save it as TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the TSV Files were preprocessed by :\n",
    "1) Tokenization\n",
    "\n",
    "2) Removing stop words\n",
    "\n",
    "3) Removing punctuation\n",
    "\n",
    "4) Stemming\n",
    "\n",
    "5) Removing [] , \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    stemmer = PorterStemmer()\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text) #devide the text into substrings\n",
    "    filtered1 = [w for w in words if not w in stop_words] #remove stop words\n",
    "    filtered2 = list(filter(lambda word: word not in string.punctuation, filtered1))\n",
    "    filtered3 = []\n",
    "    for word in filtered2:\n",
    "        try:\n",
    "            filtered3 += re.findall(r'\\w+', word) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    filtered3 = [stemmer.stem(w) for w in filtered3] #stemming\n",
    "    filtered4 = [c.replace(\"''\", \"\").replace(\"``\", \"\") for c in filtered3 ] #removing useless '' and  `` characters\n",
    "    filtered4 = [f for f in filtered4 if len(f)>1]\n",
    "    return filtered4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) creating index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we should first create a dictionary with all the words in our documents. The keys of this dictionary are integers(term_ids) and values are words.\n",
    "Another dictionary that we create is docwords which points each document to list of all words in that document.\n",
    "Another dictionary is tsvs which contains intro and plot section of each document.\n",
    "we save these dictionaries  as json files to use afterwards in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function save an object to desired path as a json file\n",
    "def savetojson(pathfile, obj):\n",
    "    with open(pathfile, \"w\" ,encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(json.dumps(obj, ensure_ascii = False))\n",
    "        out_file.close()\n",
    "        \n",
    "        \n",
    "def get_vocab_index(path) :\n",
    "    allwords = list()\n",
    "    docwords = dict() # point each document to its containing words\n",
    "    tsvs = dict()\n",
    "    vocabulary = dict() # point each term id to a word\n",
    "    for i in range(0,30000):\n",
    "        with open(path+\"\\\\TSV\\\\article_\" + str(i) + \".tsv\", encoding = \"utf-8\") as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row :\n",
    "                    tsv = row\n",
    "        text = ' '.join([tsv[1],tsv[2]]) #get intro and plot of each tsv file\n",
    "        tsvs[i] = tsv\n",
    "        cleared = clean(text)\n",
    "\n",
    "        docwords['document_'+str(i)] = cleared\n",
    "        allwords += cleared\n",
    "        \n",
    "        \n",
    "    allwords = list(set(allwords)) # get the list of unique words\n",
    "        for i in range(len(allwords)):\n",
    "            vocabulary[str(i)] = allwords[i]\n",
    "            \n",
    "            \n",
    "            \n",
    "    savetojson(path+\"\\\\tsvs.json\", tsvs)\n",
    "    savetojson(path + \"\\\\WORDS\\\\DocWords.json\", docwords)\n",
    "    savetojson(path + \"\\\\WORDS\\\\vocabulary.json\", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should create inverted_index which points each term_id to the documents that contains that word. First we load vocabulary json file that we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(path) :\n",
    "    inverted = defaultdict(list)\n",
    "    \n",
    "    with open(path + \"\\\\WORDS\\\\vocabulary.json\", encoding = \"utf-8\") as fd:\n",
    "        vocabulary = json.load(fd)\n",
    "        \n",
    "    reverse_voc = {v:k for k,v in vocabulary.items()} # we need to inverse keys and values of our dictionary\n",
    "    \n",
    "# we check for each document and for each word in that doument whether that document exist in inverted dictionary\n",
    "#or not, and if it didn't exist we add the document number\n",
    "    for doc in docwords.keys():\n",
    "        for word in docwords[doc]:\n",
    "            if not doc in inverted[reverse_voc[word]]:\n",
    "                inverted[reverse_voc[word]].append(doc)\n",
    "                \n",
    "    savetojson(path + \"\\\\WORDS\\\\Inverted_index.json\", inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) execute query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the query from user and replace each word with the term_id. If the word did not exist in vocabulary dictionary we assign NA to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query():\n",
    "    query = input(\"Insert your query: \")\n",
    "    return(clean(query))\n",
    "\n",
    "def get_query_index(query) :\n",
    "    indexes = []\n",
    "    for i in range(len(query)) :\n",
    "        if query[i] in vocabulary.values() : #if the vocab in query exist in vocabulary dataset\n",
    "            indexes.append(reverse_voc[query[i]]) #add term_id of that vocab to query\n",
    "\n",
    "        else : #if it does not exist in vocabulary we replace it with 'NA'\n",
    "            indexes.append('NA')\n",
    "    return(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we should find the documents that contain all words of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    if len(query) == 0:\n",
    "        return('Please, insert text in your search')\n",
    "    query = get_query_index(query)\n",
    "    docs = []\n",
    "    for i in query :\n",
    "        if (i == 'NA') : \n",
    "#if there is a vocab in query that does not exist in vocabulary dataset, there isn't a match and we should terminate the function\n",
    "            return(\"No match for your query\")\n",
    "        else :\n",
    "            docs.append(set(inverted_index[i]))\n",
    "        \n",
    "    docs = set.intersection(*docs)\n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we create some functions that we need to run and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linked_URL(val): #we will use this to make the urls in output clickable\n",
    "        # target _blank to open new window\n",
    "        return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "    \n",
    "def replacer(val):      #This is used to escape the character $ in the output for Intro,\n",
    "    return val.replace('$', '\\$')      #otherwise it would be interpreted by displayer\n",
    "\n",
    "def Run_SE1():\n",
    "    query = get_query()\n",
    "    results = []\n",
    "    for file in execute_query(query):\n",
    "        docid = file.split('_')[1]\n",
    "        tsv = newdict[docid]\n",
    "        results.append([docid,tsv[0],tsv[1],Movies[docid]])  #create movies file before\n",
    "    df = pd.DataFrame(results, columns = ['Id','Title', 'Intro', 'Wikipedia Url'])\n",
    "    f = open(path + '\\\\display.html','w', encoding = 'utf-8')\n",
    " \n",
    "    message = df.style.format({'Wikipedia Url': Linked_URL}).format({'Intro': replacer}).render()\n",
    "\n",
    "    f.write(message)\n",
    "    f.close()\n",
    "\n",
    "    #Change path to reflect file location\n",
    "    filename = path + '\\\\display.html'\n",
    "    webbrowser.open_new_tab(filename) # for showing the results in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(query):\n",
    "    results = []\n",
    "    for file in execute_query(query):\n",
    "        docid = file.split('_')[1]\n",
    "        tsv = newdict[docid]\n",
    "        results.append([docid,tsv[0],tsv[1],Movies[docid]])  #create movies file before\n",
    "    result = pd.DataFrame(results, columns = ['Id','Title', 'Intro', 'Wikipedia Url'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we should give scores based on cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    cosine_distance = spatial.distance.cosine\n",
    "    return 1 - cosine_distance(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we need now is to calculate the IDF and TF - IDF, according to the formulas: \n",
    "- $TF = \\frac{N_{(x,y)}}{N_{(*,y)}}$\n",
    "- $IDF = log[1 + (\\frac{D}{D_x})]$ <fr>\n",
    "\n",
    "Where:\n",
    "- $N_{(x,y)}$ is the number of times that the word $X$ is in the document $D_y$;\n",
    "- $N_{(*,y)}$ is the total number of the words in the document;\n",
    "- $D$ is the total number of documents;\n",
    "- $D_x$ is the number of documents in which the word $X$ appears at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"\\\\WORDS\\\\Inverted_index.json\", encoding = \"utf-8\") as fd:\n",
    "        inverted_index = json.load(fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"\\\\WORDS\\\\DocWords.json\", encoding = \"utf-8\") as fd:\n",
    "        docwords = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFs = dict()\n",
    "inv_ind_tfIDF = defaultdict(list)\n",
    "for term in inverted_index.keys() :\n",
    "        IDFs[term] = log(1+ 30000/len(inverted_index[term])) #first we calculat IDF for each term_id\n",
    "        for doc in inverted_index[term] :\n",
    "            tf = docwords[doc].count(vocabulary[term]) / len(docwords[doc])\n",
    "            tfidf = tf * IDFs[term]\n",
    "            inv_ind_tfIDF[term].append((doc,round(tfidf, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetojson(path + \"\\\\WORDS\\\\TfIdf_inv_index.json\", inv_ind_tfIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2)Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we should define some variables to calculate the new scores based on them. The variables that we decided to use are : The release Year, length of the movie(Run time), Budget and number of stars as these variables seems to be more important to most of users. First we get  some queries from user and based on maximum and minimum value of these varaibles among resulted documents of the first search engine we define a scoring function for each variable that gives a score between 0 and 1. Finally we calculate the mean of these scores and put them in a heap structure to find 10 documents that have most scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting query from user\n",
    "def get_query_SE3():\n",
    "    query = input(\"insert your query : \")\n",
    "    query = clean(query)\n",
    "    q = dict()\n",
    "    \n",
    "    year = input(\"Do you want to specify the release year ? [Y/N] : \").lower()\n",
    "    if year == \"y\" :\n",
    "        year = input(\"Please, specify the release date : \") \n",
    "        q[\"year\"] = year\n",
    "    else:\n",
    "        q[\"year\"] = 'NA'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    Runtime = input(\"Do you want to specify the length of the movie? [Y/N] : \").lower()\n",
    "    if Runtime == \"y\" :\n",
    "        Runtime = input(\"Please, specify the length of the movie : \")\n",
    "        if re.search('\\d', Runtime):\n",
    "            q['Runtime'] = Runtime\n",
    "        else:\n",
    "            return 'Please, enter a valid runtime.'\n",
    "    else :\n",
    "        q[\"Runtime\"] = 'NA'\n",
    "\n",
    "\n",
    "    starring = input(\"Is number of stars an important factor for you? [Y/N] : \").lower()\n",
    "    if starring == \"y\" :\n",
    "        starring = input(\"Please, specify if you're looking for a big or small cast [B/S]: \")\n",
    "        q[\"starring\"] = starring\n",
    "    else :\n",
    "        q[\"starring\"] = 'NA'\n",
    "\n",
    "\n",
    "    budget = input(\"Is movie budget an important factor for you? [Y/N] : \").lower()\n",
    "    if budget == \"y\" :\n",
    "        q['Budget'] = input(\"Please, specify the budget of the movie you're looking for : \")\n",
    "    else :\n",
    "        q['Budget'] = 'NA'\n",
    "        \n",
    "    return query,q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should execute our search engine with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine3() :\n",
    "    (query, q) = get_query_SE3()\n",
    "    results = execute_query(query)    #running the first search engine to get all query_related documents \n",
    "                                    # Now we should define variables that we want to use to give a new score\n",
    "    d = defaultdict(dict)\n",
    "    result_variables = dict() # A dictionary that assigns each document to a dictionary of variables in that document\n",
    "     # A dictionary that\n",
    "    for i in results :\n",
    "        docId = i.split(\"_\")[1] \n",
    "        tsv = newdict[docId]\n",
    "\n",
    "\n",
    "        d[i] = dict()\n",
    "\n",
    "        if tsv[6] == 'NA':\n",
    "            d[i]['Starring'] = '-10000'\n",
    "        else:\n",
    "            d[i]['Starring'] = str(len(tsv[6].replace('\\n', '').strip(',').split(',,')))\n",
    "\n",
    "        try:\n",
    "            d[i]['Release Year'] = re.search(r'\\d{4}', tsv[8]).group(0)\n",
    "        except:\n",
    "            d[i]['Release Year'] = '-10000'\n",
    "\n",
    "        try:\n",
    "            d[i]['Runtime']    = re.search(r'\\d+.*',tsv[9]).group(0)\n",
    "        except:\n",
    "            d[i]['Runtime']    = '-10000'\n",
    "\n",
    "        #some movies have running time expressed in reels, and the conversion in minutes is not univoque, so we'll just ignore those info\n",
    "        if re.search(r'min', d[i]['Runtime']):\n",
    "            d[i]['Runtime'] = re.search(r'\\d+[\\.|\\,|:]*\\d*', d[i]['Runtime']).group(0)\n",
    "            d[i]['Runtime'] = re.search(r'\\d+', d[i]['Runtime']).group(0)\n",
    "        else:\n",
    "            d[i]['Runtime'] = '-10000'\n",
    "\n",
    "        try:\n",
    "            d[i]['Budget']   = re.findall(r'\\$.*', tsv[12])[0]\n",
    "        except:\n",
    "            d[i]['Budget']  = '-10000'\n",
    "\n",
    "\n",
    "        if re.search(r'mil', d[i]['Budget']):\n",
    "            d[i]['Budget']  = str(int(float(re.search(r'\\d+[\\.|\\,]*\\d*', d[i]['Budget']).group(0).replace(',', '.'))*10**6))\n",
    "\n",
    "        elif re.search(r'\\,', d[i]['Budget']) or re.search(r'\\.', d[i]['Budget']):\n",
    "            d[i]['Budget'] = re.search(r'(\\d+[\\,!\\.])+\\d+', d[i]['Budget']).group(0).replace(',', '').replace('.', '')\n",
    "\n",
    "\n",
    "        result_variables[docId] = d[i]\n",
    "\n",
    "        Runtimes = []\n",
    "\n",
    "    Release_year = []\n",
    "    Starring = []\n",
    "    Budget = []\n",
    "\n",
    "    for i in result_variables.keys() :\n",
    "        i = 'document_'+str(i)\n",
    "        Runtimes.append(int(d[i][\"Runtime\"]))\n",
    "        Release_year.append(int(d[i][\"Release Year\"]))\n",
    "        Starring.append(int(d[i][\"Starring\"]))\n",
    "        Budget.append(int(d[i][\"Budget\"]))\n",
    "    scores = dict()\n",
    "    for i in result_variables :\n",
    "        # calculating score for Running time\n",
    "        i = 'document_'+ str(i)\n",
    "        minrun = min(Runtimes)\n",
    "        maxrun = max(Runtimes)\n",
    "        if re.search('\\d', q['Runtime']):\n",
    "            run_score = exp(-(int(re.search('\\d+', q['Runtime']).group(0)) -int(d[i]['Runtime']))**2/100)\n",
    "        else:\n",
    "            run_score = 0\n",
    "\n",
    "\n",
    "       # calculating score for quantitative Release_year query\n",
    "        if re.search('\\d', q['year']):\n",
    "            distance = abs(int(d[i]['Release Year']) - int(re.search('\\d+',q[\"year\"]).group(0)))\n",
    "            year_score = exp(-distance/10)\n",
    "        else:\n",
    "            year_score = 0\n",
    "\n",
    "\n",
    "      # calculating score for budget\n",
    "\n",
    "        if re.search('\\d', q['Budget']):\n",
    "            if re.search(r'mil', q['Budget']):\n",
    "                Budget  = int(float(re.search(r'\\d+[\\.|\\,]*\\d*', q['Budget']).group(0).replace(',', '.'))*10**6)\n",
    "\n",
    "            elif re.search(r'\\,', q['Budget']) or re.search(r'\\.', q['Budget']):\n",
    "                Budget = int(re.search(r'(\\d+[\\,!\\.])+\\d+', q['Budget']).group(0).replace(',', '').replace('.', ''))\n",
    "\n",
    "\n",
    "            budget_score = exp(-abs(int(Budget) - int(d[i]['Budget'])) / 10**5)\n",
    "        else:\n",
    "            budget_score = 0\n",
    "\n",
    "    # calculating score for starring\n",
    "        maxstar = max(Starring)\n",
    "        minstar = min(Starring)\n",
    "        if q['starring'] == 'B':\n",
    "            starring_score = (maxstar - int(d[i]['Starring']))/(maxstar-minstar)\n",
    "        elif q['starring'] == 'S':\n",
    "            starring_score = (int(d[i]['Starring']) - minstar)/(maxstar-minstar)\n",
    "        else:\n",
    "            starring_score = 0\n",
    "\n",
    "        mean_score = 1/4 * (run_score + year_score + budget_score + starring_score)\n",
    "        scores[i] = (mean_score, i)\n",
    "        \n",
    "# use heap structure to sfine the 10 best score\n",
    "    heap = []\n",
    "    for doc in scores:\n",
    "        heapq.heappush(heap, scores[doc])\n",
    "    heap_result = heapq.nlargest(10, heap)\n",
    "    df = dict()\n",
    "    for x,z in heap_result:\n",
    "        y = z.split('_')[1]\n",
    "        df[y] = newdict[y][0:2]\n",
    "        df[y].append(Movies[y])\n",
    "        df[y].append(x)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df, orient = 'index', columns=['Title', 'Intro', 'Wikipedia UrlL', 'Score'])\n",
    "    f = open(path + '\\\\display.html','w', encoding = 'utf-8')\n",
    " \n",
    "    message = df.style.format({'Wikipedia Url': Linked_URL}).format({'Intro': replacer}).render()\n",
    "\n",
    "    f.write(message)\n",
    "    f.close()\n",
    "\n",
    "    #Change path to reflect file location\n",
    "    filename = path + '\\\\display.html'\n",
    "    webbrowser.open_new_tab(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Create a co-stardom network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from Functions import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should insert a query and get the results from the third search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(query, q) = get_query_SE3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, q = (['orc', 'elv'],\n",
    " {'year': '1995', 'Runtime': 'NA', 'starring': 'NA', 'Budget': '20 milions'})\n",
    "result = search_engine3(query, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdict['14018'] #from the functions import, newdict is the dictionary of all tsv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [] #set of star actors in 10 first result of search engine\n",
    "for ind in result.index :\n",
    "    tsv = newdict[str(ind)]\n",
    "    for i in tsv[6].replace('\\n', '').strip(',').split(',,') : #stars of a movie\n",
    "        if i not in stars :\n",
    "            stars.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have nodes of our network. In order to create edges, first we shoud make a list of all possible duel combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duel_stars = []\n",
    "for i in range(0 , len(stars)) :\n",
    "    for j in range(i+1,len(stars)) :\n",
    "        duel_stars.append((stars[i],stars[j])) #make a nested list with all possible duel combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we should check which duel combinations that we created in previous step, exist in more than 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = []\n",
    "nodes = set()\n",
    "for x,y in duel_stars : #for each dual combination\n",
    "    counter = 0\n",
    "    for j in result.index: # search in the whole dataset\n",
    "        tsv = newdict[str(j)]\n",
    "        starring = tsv[6].replace('\\n', '').strip(',').split(',,')\n",
    "        if len(set((x,y)).intersection(set(starring))) == 2:\n",
    "            counter += 1\n",
    "            if counter == 2:\n",
    "                edge.append((x,y))\n",
    "                nodes.add(x)\n",
    "                nodes.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have Nodes and Edges. It's time to create a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(stars)\n",
    "G.add_edges_from(edge)\n",
    "nx.draw(G, with_labels = 5)\n",
    "plt.savefig(\"co-stardom.png\", format = 'png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to have better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(stars)\n",
    "G.add_edges_from(edge)\n",
    "pos = dict()\n",
    "\n",
    "c = 0\n",
    "\n",
    "for i in set(stars)-nodes:\n",
    "    c+=1\n",
    "    if c%2 ==0:\n",
    "        m = 0\n",
    "    else:\n",
    "        m = 1\n",
    "    pos[i] = (m*250, c*15)\n",
    "    \n",
    "for i in nodes:\n",
    "    pos[i] = (random.choice(range(50,200)), random.choice(range(1,c*15)))\n",
    "nx.draw(G, pos, with_labels=True, font_color = 'red', font_size = 13, font_weight = 'bold')\n",
    "plt.xlim([-70, 320])\n",
    "plt.savefig(\"co-stardom.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis on this network. We can make a density distribution histogram which shows the number of connections for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_degrees =[val for (node, val) in G.degree()]\n",
    "unique_degrees = list(set(all_degrees))\n",
    "count = []\n",
    "for i in unique_degrees :\n",
    "    x = all_degrees.count(i)\n",
    "    count.append(x)\n",
    "plt.plot(unique_degrees , count, \"yo-\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Number of nodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate degree centality for each node to see which actors where more involved with other actors(most influential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.degree_centrality(G), key = nx.degree_centrality(G).get , reverse = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
