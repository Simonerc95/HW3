{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nino'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    file = open(\"C:\\\\Users\\\\Nino\\\\ADM_HW3\\\\articl_\" + str(i) + \".tsv\",\"r\",encoding = \"utf-8\") \n",
    "    data = file.read()\n",
    "        \n",
    "len(data.split(' ').count())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1) create index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for creating inverted index, we first need to have a list of all words in intro and plot sections of our documents. We create a json file which assign each word to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the list of all the words\n",
    "vocab = []\n",
    "for i in range(0,1):\n",
    "    file = open(\"C:\\\\Users\\\\Nino\\\\ADM_HW3\\\\articl_\" + str(i) + \".tsv\",\"r\",encoding = \"utf-8\") \n",
    "    data = file.read()\n",
    "    for j in data.split(\" \") :\n",
    "        vocab.append(j)\n",
    "        \n",
    "uniVocab = list(set(vocab))\n",
    "\n",
    "vocabulary = dict() #create empty dictionary\n",
    "for i in uniVocab:\n",
    "    vocabulary[i] = uniVocab.index(i) # build a dicyionary that map each word to a term_id\n",
    "    \n",
    "#we save the vocabulary as json file\n",
    "with open(\"vocabulary.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(vocabulary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Warren Stacy (Gene Davis) is a young office equipment repairman who kills women after they reject his sexual advances. His attempts at flirting are always seen as \"\"creepy\"\" by women, resulting in frequent rejections.[4] His first victim is Betty (June Gilbert), an office worker of his acquaintance. He tracks her down to a wooded area, and observes her having sex with her boyfriend. He ambushes the couple, kills the boyfriend and then gives chase to the naked woman. He catches her and stabs her to death.[4]\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.split(\"\\t\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = dict() #make an empty dictionart to put inversed_index in it\n",
    "for i in range(0,10):\n",
    "    file = open(\"C:\\\\Users\\\\Nino\\\\ADM_HW3\\\\articl_\" + str(i) + \".tsv\",\"r\",encoding = \"utf-8\") \n",
    "    data = file.read()\n",
    "    for item in set(data.split(\" \")): #for each unique word in our data\n",
    "        if item not in inverse.values() : # if key = item does not already exist in the dictionary\n",
    "            inverse[item] = i #create key = item and assign value i to it\n",
    "        else: # if key = item already exist in the dictionary\n",
    "            inverse[item].append(i) # add value = i to the list of values\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have inverse dictionary which assign each unique vocabulary to list of documents that contain that vocab. The last thing we should do in this step is to change the key of this dictionary with the term_ids(instead of vocabulary we should have term_id as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocabulary:\n",
    "    inverse[i] = inverse[vocabulary[i]] #vocabulary[i] is the term_id\n",
    "    del inverse[vocabulary[i]] #after changing key we should delete that element\n",
    "#save inverted index as json\n",
    "with open(\"inverted_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load vocabulary and inverted_index json files that we created in the previous step\n",
    "vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "vocabulary = {k:v for v,k in vocabulary.items()}\n",
    "inverted_index = json.loads(open(\"inverted_index.json\").read())\n",
    "\n",
    "\n",
    "query = input(\"Insert your query: \")\n",
    "# clean query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we define a function to get the query and return term_id of each vocab in query if they exist in vocabulary dataset\n",
    "def get_query_index(query) :\n",
    "    for i in range(len(query)) :\n",
    "        if query[i] in vocabulary : #if the vocab in query exist in vocabulary dataset\n",
    "            query.append(vocabulary[query[i]]) #add term_id of that vocab to query\n",
    "            query.remove(query[i])# and remove the vocab\n",
    "\n",
    "        else : #if it does not exist in vocabulary we replace it with 0\n",
    "            query[i] = 0\n",
    "    return(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to execute the query. First we should build a list that contains set of inverted_indexes of query elements and then since we want documents that contain all words of query we should find the intesection of these sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    query = get_query_index(query)\n",
    "    for i in query :\n",
    "        if (i == 0) : \n",
    "#if there is a vocab in query that does not exist in vocabulary dataset, there isn't a match and we should terminate the function\n",
    "            return(\"No match for your query\")\n",
    "        else :\n",
    "            docs.append(inverted_index[i])\n",
    "        \n",
    "    docs = set.intersection(*docs)\n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for file in docs:\n",
    "    opentsv = open( \"C:\\\\Users\\\\Nino\\\\ADM_HW3\\\\articl_\" + str(file) + \".tsv\", \"r\", encoding=\"utf8\")\n",
    "    data = file.read()\n",
    "    result.append([data[0],data[1],movies[file]])  #create movies file before\n",
    "    opentsv.close()\n",
    "result = pd.DataFrame(results, columns = ['Title', 'Intro', 'Wikipedia Url'])\n",
    "result.style.format({'Url': make_clickable})  # to get clickable urls     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2)Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we should first calculate TF-IDF for each id in inverted_index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "vocabulary = {k:v for v,k in vocabulary.items()}\n",
    "inverted_index = json.loads(open(\"inverted_index.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = []\n",
    "for term in inverted_index :\n",
    "        idf = math.log(1+ 30000/len(inverted_index[term]))\n",
    "        for doc in term :\n",
    "            file = open(\"C:\\\\Users\\\\Nino\\\\ADM_HW3\\\\articl_\" + str(doc) + \".tsv\",\"r\",encoding = \"utf-8\") \n",
    "            data = file.read()\n",
    "            tf = data.split(\" \").count(vocabulary[term]) / len(data.split(\" \"))\n",
    "            tfidf = tf * idf\n",
    "            inverted_index[term][inverted_index[term].index(doc)]=(doc,tfidf)\n",
    "           \n",
    "            \n",
    "            \n",
    "with open(\"tfidf_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "    myfile.write(json.dumps(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    return(1 - spatial.distance.cosine(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querydf(query):\n",
    "    \n",
    "    qtf = dict()\n",
    "    for word in query :\n",
    "        term_id = vocabulary[word]\n",
    "        try :\n",
    "            qtf[\"term_id\"] = \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querytf(query):\n",
    "    qtf = dict()\n",
    "    for word in query :\n",
    "        term_id = vocabulary[word]\n",
    "        try :\n",
    "            qtf[\"term_id\"] += 1/len(query)\n",
    "            \n",
    "        except :\n",
    "            \n",
    "            qtf[\"term_id\"] = 1/len(query)\n",
    "    return(qtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = input(\"insert your text : \")\n",
    "\n",
    " # clean query\n",
    "\n",
    "vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "tfidf = json.loads(open(\"tfidf_index.json\").read())\n",
    "docs = []\n",
    "for i in range(len(query)) :\n",
    "    if query[i] in vocabulary :\n",
    "        query[i] = vocabulary[query[i]] #change the words in the query with their term_id\n",
    "        result.append(inverted_index(query[i]))  \n",
    "        \n",
    "result = set.intersection(*docs)  \n",
    "\n",
    "    else :\n",
    "        return(\"Nothing found ! \")\n",
    "    \n",
    "    \n",
    "\n",
    "def execute(query) :\n",
    "    wordtf = defaultdict(list)\n",
    "    for term in query :\n",
    "        for elem in tfidf[term]:\n",
    "            if elem[0] in result:\n",
    "                wordtf[elem[0]].append(elem[1])\n",
    "                \n",
    "                \n",
    "    \n",
    "        \n",
    "query = querytf(query)\n",
    "list_query = list(query.values())\n",
    "\n",
    "\n",
    "heap = []\n",
    "    for doc in wordtf:\n",
    "    heapq.heappush(heap, (cosine_similarity(list_query, wordtf[doc]), doc))\n",
    "    \n",
    "    \n",
    "    \n",
    "heap_result = heapq.nlargest(10, heap)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
